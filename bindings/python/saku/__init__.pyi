from typing import List

class SentenceTokenizer:
    """
    """

    def __init__(self, eos: str = None, patterns: List[str] = None):
        pass

    def tokenize(self, document: str) -> List[str]:
        pass

    def tokenize_raw(self, document: str) -> List[str]:
        pass